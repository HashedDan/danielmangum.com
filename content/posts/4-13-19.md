+++ 
draft = true
date = 2019-04-13T14:39:17-05:00
title = "Understanding ETL to ELT by Going to Costco"
slug = "etl-to-elt" 
tags = []
categories = []
+++

If you have dealt with any data platform in the last few years, you have likely heard about the movement from traditional ETL (Extract Transform Load) to ELT (Extract Load Transform). The name ELT is self-explanatory as the sequential order of tasks is switched such that loading comes before the transformation, but it takes a little more investigation to understand why the movement has taken place and how it is even possible. To illustrate the evolving architecture of data platforms in the last decade, we will take a look at the difference between Costco and McDonalds. 

## The Old Model (ETL)

In the past, the data warehouse has been a place to *consume* data. Whether it was viewed through a business intelligence tool, consumed through an API, or displayed in some other application, the data was uniformly structured and structured for easy consumption. Very little (if any) modification was done to the data within the actual warehouse. It looked very similar to a restaurant. In most places in America, you can reliably go to McDonalds and get a Big Mac that is generally the same as a Big Mac you would get at any other McDonalds. Do you know why? It's because the Big Mac is only lightly prepared at the actual restaurant. Most of the work is done before the meal arrives, and the Big Mac you have in Los Angeles might even come from the same place as the one you have in Miami. There is a factory somewhere that acquires the food, turns it into a Big Mac, freezes it, and sends it to each individual McDonalds.

Why is this a bad thing? Well it isn't inherently bad, just like the old data warehouse model isn't inherently bad. Much like McDonalds, data warehouses have traditionally receieved their data already prepared. The reception is the *Load* part of ETL, the final step. But where do the first two steps happen? Again like McDonalds, at the "factory". ETL tools, such as Oracle Data Integrator or SAP, target all kinds of data sources (databases, flat files, etc.). They first *Extract* the data, then *Transform* it, before *Loading* it into the data warehouse.

The transformation process is arguable the most important part of the process, and also the most computationally expensive. Therefore, these ETL tools have to be able to access vast amounts of compute, and usually run on servers optimized for those types of tasks. The data warehouse, on the other hand, was not constructed to do heavy computation. It is optimized for storage and is not easily scalable. This made sense when companies with lots of data were buying their own servers, and managing their own data centers. It could take months to acquire a new server, and the databases used for a data warehouse were not built to be distributed across multiple servers.

This setup is logical, given the constraints of the time, but it leads to potentially stale data and log wait times between data creation in a source system and its appearance in BI tools that consume from the data warehouse.

## The New Model (ELT)

The old model has evolved in direct correlation to the rise of cloud computing. When we look back at the constraints that led to the setup of the traditional data warehouse, it is apparent that many of them have been solved by cloud provider managed services.