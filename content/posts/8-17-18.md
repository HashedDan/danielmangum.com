+++ 
draft = false
date = 2018-08-17T07:18:31-06:00
title = "Provisioning a Consul / Nomad Cluster on Google Cloud with Terraform"
slug = "consul-nomad-on-gcp-terraform" 
tags = []
categories = []
+++

*Note: All code in this tutorial is available on [GitHub](https://github.com/HashedDan/gcp-consul-nomad).*

If you are moving to a microservices architecture (or even if you are not), you are most likely familiar with at least one of Hashicorp's products. Today we are going to focus on three of the company's products: Terraform, Consul, and Nomad. This post will simply detail how to setup the cluster, but there will be further posts discussing how to use its capabilities and how to structure your clusters based on your use case.

## A Brief Overview of Each Product

[**Terraform**](https://www.terraform.io/)

We will use Terraform to design our infrastructure and automatically provision our server and client nodes. It will allow us to easily create, destroy, and iterate upon our cloud architecture.

[**Consul**](https://www.consul.io/)

In the era of microservices, the concept of *service discovery* has become vitally important. If a monolithic application is to be broken down into many small services, they must be able to find and communicate with each other. Consul also allows us automatically bootstrap our Nomad cluster.

[**Nomad**](https://www.nomadproject.io/)

Once our infrastructure is provisioned by Terraform, Nomad allows us to manage it. This includes deploying, scheduling, upgrading, monitoring, and scaling applications of any kind. A good way to think of it is a less opiononated Kubernetes.

## Let's Get Started

*Note: This post assumes that you have already installed Terraform on your local machine.*

For this guide, we will be using Google Cloud becuase of their generous free credits for new users. The first thing you need to do is go to the [Google Cloud](https://cloud.google.com/) page and setup a new account by clicking Try Free.

Next, you will need to setup a new project from the Google Cloud Console. Name it whatever you like.

Lastly, we need to setup the Google Cloud SDK so that we can interact with our project from the command line. Follow this [link](https://cloud.google.com/sdk/docs/quickstarts) to find the quickstart guide for your operating system. Make sure to select the project you created when prompted.

Now we are ready to start provisioning our infrastructure!

## Building the Modules

If you are not familiar with Terraform, it uses the concept of modules to build up an infrastructure. While there are many existing Google Cloud Terraform modules already on Github, we are going to build ours from the ground up to show how Terraform works.

Let's begin by creating a directory structure for our project:

```
/ gcp-consul-nomad
    / modules
        / cluster
        / security
    / scripts
```

In our top-level directory (gcp-consul-nomad), we will build our main Terraform configuration, making use of the modules and scripts defined in their respective directories.

### The Server / Client Model

Both Consul and Nomad use a Server / Client structure where the servers orchestrate the clients. For this post, we will be building a cluster of 3 Consul / Nomad servers, and a cluster of 2 Consul / Nomad clients. What this means is that we will provision a total of 5 compute instances, 3 of which will be running the Consul and Nomad binaries in server mode, and 2 of which will be running the Consul and Nomad binaries in client mode.

Let's start with building out the ```cluster```. Navigate to the directory and create the following two files: ```main.tf``` and ```variables.tf```. The main file is where we will build the modules, and we will define the variables used in the main file in the variables file.

There are a couple of ways you could go about provisioning compute instances on GCP. We are going to provision ours with an *image template* and a *compute instance group manager*. Alternatively, you could manually build each compute instance. The group manager allows us to provision multiple instances from the same image template, which comes in handy when we need a collection of identical machines to be allocated.

#### Scaffolding the Compute Instance Template

Terraform refers to cloud provider services as *resources*. As previously mentioned, the first resource we are going to be building is the image template.

In the ```main.tf``` file, insert the following stanza:
```
resource "google_compute_instance_template" "server_template" {
  count   = ""
  project = ""

  name_prefix  = ""
  machine_type = ""
  region       = ""

  tags = ""

  disk {}

  network_interface {}

  metadata = ""

  service_account {}
}
```
This is the basic outline for the instance template. If you have questions about the specific properties defined within it (or what other options are available), you can look at the Google Provider [google_compute_instace_template](https://www.terraform.io/docs/providers/google/r/compute_instance_template.html) section in the Terraform documentation. However, we will go through one-by-one and populate the properties, describing why we are doing so for each property.

#### Thinking About Variables

Before we get started, one aspect of Terraform that is important to understand is the ability to use variables. Variables must be defined in a ```variables.tf``` file, and they represent values that a user may want to define. So, when creating modules, we want to think about **what parts of that module are important to assign user-defined variables to in order to make it customizable and reusable?** It is good to keep in mind that, typically, the more variables in a module, the more reusable it will be, but the more complex it will be to use as well (this is a tradeoff that you must consider when building Terraform modules for your organization).

> It is good to keep in mind that, typically, the more variables in a module, the more reusable it will be, but the more complex it will be to use as well (this is a tradeoff that you must consider when building Terraform modules for your organization).

#### Populating the Compute Instance Template

In populating the compute instance parameters, I am going to err on the side of more reusable, due to the fact that this is an open project that I want others to be able to customize as they see fit.

The first parameter is ```count```. Count refers to the number of templates that we want to be generated when this module is envoked. Remember, this does not refer to the number of compute instances, they are simply generated from this template. Therefore, the user of this module likely only wants 1 or 0 templates. Knowing this, we will write a conditional based on a variable that the user can define:
```
count   = "${var.module_enabled ? 1 : 0}"
```
Essentially this is just the user saying that they either want to generate the template or not. Now that we have defined the ```module_enabled``` variable in the main file, we must make a corresponding entry in the variables file:
```
variable module_enabled {
  description = "Denotes whether the module is to used or not."
  default     = true
}
```
We have defined a description to ensure the user understands what the variable is for, but we have also defined a default value. If a variable is defined with a default value, the user is not required to define it when using the module, and if they don't it will default to that value.

Next, we have the ```project``` parameter. Google organizes cloud resources around projects, and anything provisioned in Google Cloud Platform must be associated with a given project. This will clearly be something we want the user to define, so we will once again declare one in main.tf:
```
project = "${var.project}"
```
And in variables.tf:
```
variable project {
  description = "The project to deploy to, if not set the default provider project is used."
  default     = ""
}
```
The next parameter is ```name_prefix```. Since this resource will always create an instance template, we can make the assumption that the user of the module will always want to prefix the resource with something like "instance-template" (once again, in another situation, you may want the user of the module to be able to customize this parameter). We can simply set the value in main.tf:
```
name_prefix  = "instance-template-"
```
For the next two parameters ```machine_type``` and ```region```, we will want the user to be able to decide both the [type of machine](https://cloud.google.com/compute/docs/machine-types) and the region that it runs in for the image template. We will once again define variables in the main file:
```
machine_type = "${var.machine_type}"
region       = "${var.region}"
```
And the variables file:
```
variable "machine_type" {
  description = "Type of VM specified for each node."
  default     = "f1-micro"
}

variable region {
  description = "Region for cloud resources."
  default     = "us-central1"
}
```
The ```tags``` parameter references tags that will be assigned to any instances created using this template. We will let the module user decide this variable, and we will actually show how it can be used when we build our cluster.
In main:
```
tags = "${var.tags}"
```
In variables:
```
variable "tags" {
  description = "Tags associated with compute instance."
  default     = []
}
```
One important thing to note on this parameter is that it takes an array. We have specified the default as an empty array (i.e. no tags), but the user may provide an arbitrary number of tags.

The next set of parameters are part of the ```disk``` stanza. These specify the properties of the base image that will be running on the instances created from the template.

The ```source_image``` indicates what type of base image (see [this page](https://cloud.google.com/compute/docs/images) for information on the public image library Google Cloud provides) the template will use to build the template. We allow the user of the module to select a base image by creating a variable. Later we will define a script used to customize this base image with things we want to add on top of it.

The ```auto_delete``` and ```boot``` parameters simply specify what there names suggest: whether the disk should be auto deleted when the instance is deleted and that this is in fact a boot disk. These are normally set to true (in fact, auto_delete will default to true if not specified), so we will go ahead and label them for the user.

All together, we have added the following in the main file:
```
disk {
    source_image = "${var.source_image}"
    auto_delete  = true
    boot         = true
}
```
And in the variables file (we have provided ubuntu image as default):
```
variable "source_image" {
  description = "Image to be used for compute instances in the cluster."
  default     = "ubuntu-1804-bionic-v20180808"
}
```

We have another set up parameters for the ```network_interface``` stanza. We won't go into too much detail on the network, but essentially we want to specify the instances created from this template need to be connected to the "default" Virtual Private Cloud (VPC) for our project.

There are two parameters that we will specify: ```network``` and ```access_config```.

#### Scaffolding the Compute Instance Group Manager Template

Now that we have the compute instance template built, we want to go about scaffolding our compute instance group manager. Below our compute instance template code in ```main.tf``` insert the following stanza:
```
resource "google_compute_instance_group_manager" "default" {
  count       = ""
  project     = ""
  name        = ""
  description = ""

  base_instance_name = ""

  instance_template = ""

  zone = ""

  update_strategy = ""

  target_size = ""

  named_port {
    name = ""
    port = 
  }
}
```
This is the basic outline for the instance template. If you have questions about the specific properties defined within it (or what other options are available), you can look at the Google Provider [google_compute_instace_group_manageer](https://www.terraform.io/docs/providers/google/r/compute_instance_group_manager.html) section in the Terraform documentation.

#### Populating the Compute Instance Group Manager Template

### The Security Module

## Questions & Thoughts

If you have any questions or thoughts, please contact me on Twitter [@HashedDan](https://twitter.com/HashedDan)!